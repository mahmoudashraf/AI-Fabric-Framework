package com.ai.infrastructure.rag;

import com.ai.infrastructure.config.AIProviderConfig;
import com.ai.infrastructure.dto.AISearchRequest;
import com.ai.infrastructure.dto.AISearchResponse;
import com.ai.infrastructure.dto.VectorRecord;
import com.ai.infrastructure.exception.AIServiceException;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.document.Field;
import org.apache.lucene.document.KnnVectorField;
import org.apache.lucene.document.StringField;
import org.apache.lucene.document.TextField;
import org.apache.lucene.index.DirectoryReader;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.index.IndexWriterConfig;
import org.apache.lucene.index.Term;
import org.apache.lucene.index.VectorSimilarityFunction;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.KnnVectorQuery;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.ScoreDoc;
import org.apache.lucene.search.TopDocs;
import org.apache.lucene.search.BooleanClause;
import org.apache.lucene.search.BooleanQuery;
import org.apache.lucene.search.TermQuery;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.FSDirectory;
import org.apache.lucene.store.Lock;
import org.apache.lucene.store.LockObtainFailedException;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
import org.springframework.stereotype.Service;

import jakarta.annotation.PostConstruct;
import jakarta.annotation.PreDestroy;
import java.io.IOException;
import java.io.UncheckedIOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.stream.Collectors;

/**
 * Lucene Vector Database Service
 * 
 * This service provides vector database operations using Apache Lucene 9+ with native k-NN search.
 * Uses KnnVectorField and KnnVectorQuery for optimized approximate nearest neighbor search.
 * It's designed for development and testing environments where external vector databases
 * are not available or needed.
 * 
 * This implementation delegates similarity calculations to Lucene's native k-NN implementation,
 * making it swappable with production vector databases (Pinecone, Qdrant, etc.) that also handle
 * similarity internally.
 * 
 * @author AI Infrastructure Team
 * @version 2.0.0
 */
@Slf4j
@RequiredArgsConstructor
@ConditionalOnProperty(name = "ai.vector-db.type", havingValue = "lucene", matchIfMissing = true)
public class LuceneVectorDatabaseService implements VectorDatabaseService {
    
    private final AIProviderConfig config;
    
    @Value("${ai.vector-db.lucene.index-path:./data/lucene-vector-index}")
    private String indexPath;
    
    @Value("${ai.vector-db.lucene.similarity-threshold:0.7}")
    private double similarityThreshold;
    
    @Value("${ai.vector-db.lucene.max-results:100}")
    private int maxResults;
    
    @Value("${ai.vector-db.lucene.vector-dimension:1536}")
    private int vectorDimension;
    
    private static final String VECTOR_FIELD = "vector";
    private static final String VECTOR_ID_FIELD = "vectorId";
    private static final String ENTITY_ID_FIELD = "entityId";
    private static final String ENTITY_TYPE_FIELD = "entityType";
    
    private static final Map<Path, SharedIndex> INDEX_CACHE = new ConcurrentHashMap<>();
    private static final ObjectMapper OBJECT_MAPPER = new ObjectMapper();

    private Directory directory;
    private IndexWriter indexWriter;
    private IndexReader indexReader;
    private IndexSearcher indexSearcher;
    private StandardAnalyzer analyzer;
    private SharedIndex sharedIndex;
    private Path resolvedIndexPath;
    
    @PostConstruct
    public void initialize() {
        try {
            log.info("Initializing Lucene Vector Database at: {}", indexPath);
            
            // Create index directory if it doesn't exist
            resolvedIndexPath = Paths.get(indexPath).toAbsolutePath().normalize();
            if (!Files.exists(resolvedIndexPath)) {
                Files.createDirectories(resolvedIndexPath);
            }

            analyzer = new StandardAnalyzer();
            sharedIndex = INDEX_CACHE.compute(resolvedIndexPath, (path, existing) -> {
                if (existing != null) {
                    existing.retain();
                    return existing;
                }

                try {
                    Directory newDirectory = FSDirectory.open(path);
                    IndexWriterConfig config = new IndexWriterConfig(analyzer);
                    config.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND);
                    IndexWriter writer;
                    try {
                        writer = new IndexWriter(newDirectory, config);
                    } catch (LockObtainFailedException lockException) {
                        log.warn("Existing Lucene lock detected at {}. Attempting recovery.", path, lockException);
                        writer = recoverFromLock(newDirectory, config, path);
                    }
                    return new SharedIndex(newDirectory, writer);
                } catch (IOException ioException) {
                    throw new UncheckedIOException(ioException);
                }
            });

            directory = sharedIndex.directory;
            indexWriter = sharedIndex.writer;

            // Initialize reader and searcher
            refreshReader();
            
            log.info("Lucene Vector Database initialized successfully");
            
        } catch (Exception e) {
            log.error("Failed to initialize Lucene Vector Database", e);
            throw new AIServiceException("Failed to initialize Lucene Vector Database", e);
        }
    }
    
    @PreDestroy
    public void cleanup() {
        try {
            log.debug("Closing Lucene Vector Database");
            
            if (indexWriter != null) {
                if (indexReader != null) {
                    indexReader.close();
                }
                if (resolvedIndexPath != null) {
                    if (INDEX_CACHE.computeIfPresent(resolvedIndexPath, (path, shared) -> {
                        if (shared.release()) {
                            try {
                                shared.writer.close();
                            } catch (IOException e) {
                                log.warn("Error closing IndexWriter for {}", path, e);
                            }
                            try {
                                shared.directory.close();
                            } catch (IOException e) {
                                log.warn("Error closing Directory for {}", path, e);
                            }
                            return null;
                        }
                        return shared;
                    }) == null) {
                        directory = null;
                    }
                }
            } else if (indexReader != null) {
                indexReader.close();
            }
            if (analyzer != null) {
                analyzer.close();
            }
            
            log.debug("Lucene Vector Database closed successfully");
            
        } catch (Exception e) {
            log.error("Error closing Lucene Vector Database", e);
        }
    }
    
    @Override
    public String storeVector(String entityType, String entityId, String content, 
                             List<Double> embedding, Map<String, Object> metadata) {
        try {
            log.debug("Storing vector in Lucene for entity {} of type {}", entityId, entityType);

            String vectorId = UUID.randomUUID().toString();
            Document document = buildDocument(vectorId, entityType, entityId, content, embedding, metadata);

            indexWriter.addDocument(document);
            indexWriter.commit();
            refreshReader();

            log.debug("Successfully stored vector in Lucene for entity {} of type {} with vectorId {}",
                entityId, entityType, vectorId);

            return vectorId;

        } catch (Exception e) {
            log.error("Error storing vector in Lucene", e);
            throw new AIServiceException("Failed to store vector in Lucene", e);
        }
    }
    
    @Override
    public AISearchResponse search(List<Double> queryVector, AISearchRequest request) {
        try {
            log.debug("Searching vectors in Lucene using native k-NN for query: {}", request.getQuery());
            
            long startTime = System.currentTimeMillis();
            
            // Convert query vector to float[] for Lucene k-NN
            float[] queryVectorArray = new float[queryVector.size()];
            for (int i = 0; i < queryVector.size(); i++) {
                queryVectorArray[i] = queryVector.get(i).floatValue();
            }
            
            // Use Lucene 9+ native k-NN search with KnnVectorQuery
            // This provides optimized approximate nearest neighbor search
            // The vector database handles similarity calculation internally
            int k = Math.min(request.getLimit() * 2, maxResults * 2); // Get more candidates for threshold filtering
            KnnVectorQuery vectorQuery = new KnnVectorQuery(VECTOR_FIELD, queryVectorArray, k);
            
            // Apply entity type filter if specified
            Query filterQuery = null;
            if (request.getEntityType() != null && !request.getEntityType().trim().isEmpty()) {
                filterQuery = new TermQuery(new Term(ENTITY_TYPE_FIELD, request.getEntityType()));
            }

            // Perform k-NN search (Lucene handles similarity internally)
            TopDocs topDocs;
            if (filterQuery != null) {
                BooleanQuery.Builder boolQueryBuilder = new BooleanQuery.Builder();
                boolQueryBuilder.add(vectorQuery, BooleanClause.Occur.MUST);
                boolQueryBuilder.add(filterQuery, BooleanClause.Occur.FILTER);
                topDocs = indexSearcher.search(boolQueryBuilder.build(), k);
            } else {
                topDocs = indexSearcher.search(vectorQuery, k);
            }
            
            ScoreDoc[] hits = topDocs.scoreDocs;
            
            // Process results - Lucene has already calculated similarity scores
            List<Map<String, Object>> results = new ArrayList<>();
            for (ScoreDoc hit : hits) {
                Document doc = indexSearcher.doc(hit.doc);
                
                // Lucene's k-NN search already provides similarity scores
                // The score from Lucene is the cosine similarity
                // Normalize to [0, 1] range if needed (Lucene scores are typically already normalized)
                double similarity = hit.score;
                
                // Apply threshold filter
                if (similarity >= request.getThreshold()) {
                    Map<String, Object> result = new HashMap<>();
                    result.put("id", doc.get(ENTITY_ID_FIELD));
                    result.put("vectorId", doc.get(VECTOR_ID_FIELD));
                    result.put("content", doc.get("content"));
                    result.put("entityType", doc.get(ENTITY_TYPE_FIELD));
                    result.put("metadata", doc.get("metadata"));
                    result.put("score", similarity);
                    result.put("similarity", similarity);
                    
                    results.add(result);
                    
                    // Stop once we have enough results
                    if (results.size() >= request.getLimit()) {
                        break;
                    }
                }
            }
            
            long processingTime = System.currentTimeMillis() - startTime;
            
            log.debug("Found {} results using Lucene native k-NN in {}ms", results.size(), processingTime);
            
            return AISearchResponse.builder()
                .results(results)
                .totalResults(results.size())
                .maxScore(results.isEmpty() ? 0.0 : (Double) results.get(0).get("similarity"))
                .processingTimeMs(Long.valueOf(processingTime))
                .requestId(UUID.randomUUID().toString())
                .query(request.getQuery())
                .model(config.getOpenaiEmbeddingModel())
                .build();
                
        } catch (Exception e) {
            log.error("Error searching vectors in Lucene", e);
            throw new AIServiceException("Failed to search vectors in Lucene", e);
        }
    }
    
    @Override
    public boolean removeVector(String entityType, String entityId) {
        try {
            log.debug("Removing vector from Lucene for entity {} of type {}", entityId, entityType);
            
            // Delete by entityId and entityType
            Term term = new Term(ENTITY_ID_FIELD, entityId);
            long deletedCount = indexWriter.deleteDocuments(term);
            indexWriter.commit();
            
            // Refresh reader
            refreshReader();
            
            boolean removed = deletedCount > 0;
            log.debug("Successfully removed vector from Lucene for entity {} of type {}: {}", 
                     entityId, entityType, removed);
            
            return removed;
            
        } catch (Exception e) {
            log.error("Error removing vector from Lucene", e);
            throw new AIServiceException("Failed to remove vector from Lucene", e);
        }
    }
    
    @Override
    public Map<String, Object> getStatistics() {
        try {
            Map<String, Object> stats = new HashMap<>();
            
            if (indexReader != null) {
                stats.put("totalVectors", indexReader.numDocs());
                stats.put("indexPath", indexPath);
                stats.put("similarityThreshold", similarityThreshold);
                stats.put("maxResults", maxResults);
                
                // Get entity type counts
                Map<String, Integer> entityTypeCounts = new HashMap<>();
                for (int i = 0; i < indexReader.numDocs(); i++) {
                    Document doc = indexReader.document(i);
                    String entityType = doc.get("entityType");
                    entityTypeCounts.merge(entityType, 1, Integer::sum);
                }
                stats.put("entityTypeCounts", entityTypeCounts);
                stats.put("entityTypes", entityTypeCounts.keySet());
            }
            
            return stats;
            
        } catch (Exception e) {
            log.error("Error getting Lucene statistics", e);
            return Map.of("error", "Failed to get statistics");
        }
    }
    
    @Override
    public long clearVectors() {
        try {
            log.debug("Clearing all vectors from Lucene");
            if (indexWriter == null) {
                log.debug("IndexWriter not initialized; nothing to clear");
                return 0;
            }
            
            long countBefore = indexReader != null ? indexReader.numDocs() : 0;
            indexWriter.deleteAll();
            indexWriter.commit();
            refreshReader();
            
            log.debug("Successfully cleared {} vectors from Lucene", countBefore);
            return countBefore;
            
        } catch (Exception e) {
            log.error("Error clearing vectors from Lucene", e);
            throw new AIServiceException("Failed to clear vectors from Lucene", e);
        }
    }
    
    @Override
    public boolean updateVector(String vectorId, String entityType, String entityId, 
                               String content, List<Double> embedding, Map<String, Object> metadata) {
        try {
            log.debug("Updating vector {} in Lucene for entity {} of type {}", vectorId, entityId, entityType);
            
            Term term = new Term(VECTOR_ID_FIELD, vectorId);
            long deletedCount = indexWriter.deleteDocuments(term);

            if (deletedCount == 0) {
                log.warn("Vector {} not found for update", vectorId);
                return false;
            }

            Document document = buildDocument(vectorId, entityType, entityId, content, embedding, metadata);

            indexWriter.addDocument(document);
            indexWriter.commit();
            refreshReader();

            log.debug("Successfully updated vector {} in Lucene", vectorId);
            return true;

        } catch (Exception e) {
            log.error("Error updating vector in Lucene", e);
            throw new AIServiceException("Failed to update vector in Lucene", e);
        }
    }

    private Document buildDocument(String vectorId, String entityType, String entityId, String content,
                                   List<Double> embedding, Map<String, Object> metadata) {
        Document doc = new Document();

        doc.add(new StringField(VECTOR_ID_FIELD, vectorId, Field.Store.YES));
        doc.add(new StringField(ENTITY_ID_FIELD, entityId, Field.Store.YES));
        doc.add(new StringField(ENTITY_TYPE_FIELD, entityType, Field.Store.YES));
        doc.add(new TextField("content", content, Field.Store.YES));

        float[] vectorArray = new float[embedding.size()];
        for (int i = 0; i < embedding.size(); i++) {
            vectorArray[i] = embedding.get(i).floatValue();
        }
        doc.add(new KnnVectorField(VECTOR_FIELD, vectorArray, VectorSimilarityFunction.COSINE));

        String embeddingText = embedding.stream()
            .map(String::valueOf)
            .collect(Collectors.joining(","));
        doc.add(new TextField("embedding", embeddingText, Field.Store.YES));

        if (metadata != null && !metadata.isEmpty()) {
            String metadataJson = metadata.entrySet().stream()
                .map(entry -> "\"" + entry.getKey() + "\":\"" + entry.getValue() + "\"")
                .collect(Collectors.joining(",", "{", "}"));
            doc.add(new TextField("metadata", metadataJson, Field.Store.YES));
        }

        long currentTime = System.currentTimeMillis();
        doc.add(new StringField("storedAt", String.valueOf(currentTime), Field.Store.YES));
        doc.add(new StringField("createdAt", String.valueOf(currentTime), Field.Store.YES));
        doc.add(new StringField("updatedAt", String.valueOf(currentTime), Field.Store.YES));

        return doc;
    }
    
    @Override
    public Optional<VectorRecord> getVector(String vectorId) {
        try {
            log.debug("Getting vector {} from Lucene", vectorId);
            
            Term term = new Term(VECTOR_ID_FIELD, vectorId);
            Query query = new TermQuery(term);

            TopDocs topDocs = indexSearcher.search(query, 1);
            if (topDocs.totalHits.value > 0) {
                Document doc = indexSearcher.doc(topDocs.scoreDocs[0].doc);
                return Optional.of(convertDocumentToVectorRecord(doc));
            }
            
            return Optional.empty();
            
        } catch (Exception e) {
            log.error("Error getting vector from Lucene", e);
            return Optional.empty();
        }
    }
    
    @Override
    public Optional<VectorRecord> getVectorByEntity(String entityType, String entityId) {
        try {
            log.debug("Getting vector from Lucene for entity {} of type {}", entityId, entityType);
            
            BooleanQuery.Builder builder = new BooleanQuery.Builder();
            builder.add(new TermQuery(new Term(ENTITY_TYPE_FIELD, entityType)), BooleanClause.Occur.MUST);
            builder.add(new TermQuery(new Term(ENTITY_ID_FIELD, entityId)), BooleanClause.Occur.MUST);

            TopDocs topDocs = indexSearcher.search(builder.build(), 1);
            if (topDocs.totalHits.value > 0) {
                Document doc = indexSearcher.doc(topDocs.scoreDocs[0].doc);
                return Optional.of(convertDocumentToVectorRecord(doc));
            }
            
            return Optional.empty();
            
        } catch (Exception e) {
            log.error("Error getting vector by entity from Lucene", e);
            return Optional.empty();
        }
    }
    
    @Override
    public AISearchResponse searchByEntityType(List<Double> queryVector, String entityType, 
                                              int limit, double threshold) {
        AISearchRequest request = AISearchRequest.builder()
            .query("")
            .entityType(entityType)
            .limit(limit)
            .threshold(threshold)
            .build();
        
        return search(queryVector, request);
    }
    
    @Override
    public boolean removeVectorById(String vectorId) {
        try {
            log.debug("Removing vector {} from Lucene", vectorId);
            
            Term term = new Term(VECTOR_ID_FIELD, vectorId);
            long deletedCount = indexWriter.deleteDocuments(term);
            indexWriter.commit();
            
            // Refresh reader
            refreshReader();
            
            boolean removed = deletedCount > 0;
            log.debug("Successfully removed vector {} from Lucene: {}", vectorId, removed);
            
            return removed;
            
        } catch (Exception e) {
            log.error("Error removing vector by ID from Lucene", e);
            throw new AIServiceException("Failed to remove vector by ID from Lucene", e);
        }
    }
    
    @Override
    public List<String> batchStoreVectors(List<VectorRecord> vectors) {
        try {
            log.debug("Batch storing {} vectors in Lucene", vectors.size());
            
            List<String> vectorIds = new ArrayList<>();
            
            for (VectorRecord vector : vectors) {
                String vectorId = storeVector(
                    vector.getEntityType(),
                    vector.getEntityId(),
                    vector.getContent(),
                    vector.getEmbedding(),
                    vector.getMetadata()
                );
                vectorIds.add(vectorId);
            }
            
            log.debug("Successfully batch stored {} vectors in Lucene", vectorIds.size());
            return vectorIds;
            
        } catch (Exception e) {
            log.error("Error batch storing vectors in Lucene", e);
            throw new AIServiceException("Failed to batch store vectors in Lucene", e);
        }
    }
    
    @Override
    public int batchUpdateVectors(List<VectorRecord> vectors) {
        try {
            log.debug("Batch updating {} vectors in Lucene", vectors.size());
            
            int updatedCount = 0;
            
            for (VectorRecord vector : vectors) {
                if (updateVector(
                    vector.getVectorId(),
                    vector.getEntityType(),
                    vector.getEntityId(),
                    vector.getContent(),
                    vector.getEmbedding(),
                    vector.getMetadata()
                )) {
                    updatedCount++;
                }
            }
            
            log.debug("Successfully batch updated {} vectors in Lucene", updatedCount);
            return updatedCount;
            
        } catch (Exception e) {
            log.error("Error batch updating vectors in Lucene", e);
            throw new AIServiceException("Failed to batch update vectors in Lucene", e);
        }
    }
    
    @Override
    public int batchRemoveVectors(List<String> vectorIds) {
        try {
            log.debug("Batch removing {} vectors from Lucene", vectorIds.size());
            
            int removedCount = 0;
            
            for (String vectorId : vectorIds) {
                if (removeVectorById(vectorId)) {
                    removedCount++;
                }
            }
            
            log.debug("Successfully batch removed {} vectors from Lucene", removedCount);
            return removedCount;
            
        } catch (Exception e) {
            log.error("Error batch removing vectors from Lucene", e);
            throw new AIServiceException("Failed to batch remove vectors from Lucene", e);
        }
    }
    
    @Override
    public List<VectorRecord> getVectorsByEntityType(String entityType) {
        try {
            log.debug("Getting all vectors for entity type {} from Lucene", entityType);
            
            Query query = new TermQuery(new Term(ENTITY_TYPE_FIELD, entityType));

            TopDocs topDocs = indexSearcher.search(query, Integer.MAX_VALUE);
            List<VectorRecord> vectors = new ArrayList<>();
            
            for (ScoreDoc hit : topDocs.scoreDocs) {
                Document doc = indexSearcher.doc(hit.doc);
                vectors.add(convertDocumentToVectorRecord(doc));
            }
            
            log.debug("Found {} vectors for entity type {} in Lucene", vectors.size(), entityType);
            return vectors;
            
        } catch (Exception e) {
            log.error("Error getting vectors by entity type from Lucene", e);
            throw new AIServiceException("Failed to get vectors by entity type from Lucene", e);
        }
    }
    
    @Override
    public long getVectorCountByEntityType(String entityType) {
        try {
            Query query = new TermQuery(new Term(ENTITY_TYPE_FIELD, entityType));

            TopDocs topDocs = indexSearcher.search(query, 0); // Only count, don't retrieve
            return topDocs.totalHits.value;
            
        } catch (Exception e) {
            log.error("Error getting vector count by entity type from Lucene", e);
            return 0;
        }
    }
    
    @Override
    public boolean vectorExists(String entityType, String entityId) {
        try {
            if (indexSearcher == null) {
                log.debug("IndexSearcher not initialized; vector for entity {} of type {} does not exist", entityId, entityType);
                return false;
            }
            BooleanQuery.Builder builder = new BooleanQuery.Builder();
            builder.add(new TermQuery(new Term(ENTITY_TYPE_FIELD, entityType)), BooleanClause.Occur.MUST);
            builder.add(new TermQuery(new Term(ENTITY_ID_FIELD, entityId)), BooleanClause.Occur.MUST);

            TopDocs topDocs = indexSearcher.search(builder.build(), 1);
            return topDocs.totalHits.value > 0;
            
        } catch (Exception e) {
            log.error("Error checking if vector exists in Lucene", e);
            return false;
        }
    }
    
    @Override
    public long clearVectorsByEntityType(String entityType) {
        try {
            log.debug("Clearing all vectors for entity type {} from Lucene", entityType);
            if (indexWriter == null) {
                log.debug("IndexWriter not initialized; nothing to clear for entity type {}", entityType);
                return 0;
            }
            
            Query query = new TermQuery(new Term(ENTITY_TYPE_FIELD, entityType));

            long countBefore = indexSearcher.count(query);
            indexWriter.deleteDocuments(new Term(ENTITY_TYPE_FIELD, entityType));
            indexWriter.commit();
            refreshReader();
            
            log.debug("Successfully cleared {} vectors for entity type {} from Lucene", countBefore, entityType);
            return countBefore;
            
        } catch (Exception e) {
            log.error("Error clearing vectors by entity type from Lucene", e);
            throw new AIServiceException("Failed to clear vectors by entity type from Lucene", e);
        }
    }
    
    /**
     * Convert Lucene document to VectorRecord
     */
    private VectorRecord convertDocumentToVectorRecord(Document doc) {
        try {
            String embeddingText = doc.get("embedding");
            List<Double> embedding = null;
            if (embeddingText != null && !embeddingText.trim().isEmpty()) {
                embedding = Arrays.stream(embeddingText.split(","))
                    .map(String::trim)
                    .map(Double::parseDouble)
                    .collect(Collectors.toList());
            }
            
            Map<String, Object> metadata = new HashMap<>();
            String metadataJson = doc.get("metadata");
            if (metadataJson != null && !metadataJson.trim().isEmpty()) {
                metadata.put("raw", metadataJson);
                try {
                    Map<String, Object> parsed = OBJECT_MAPPER.readValue(metadataJson, new TypeReference<Map<String, Object>>() {});
                    metadata.putAll(parsed);
                } catch (Exception parseException) {
                    log.warn("Unable to deserialize metadata JSON: {}", metadataJson, parseException);
                }
            }
            
            return VectorRecord.builder()
                .vectorId(doc.get("vectorId"))
                .entityType(doc.get("entityType"))
                .entityId(doc.get("entityId"))
                .content(doc.get("content"))
                .embedding(embedding)
                .metadata(metadata)
                .createdAt(parseTimestamp(doc.get("createdAt")))
                .updatedAt(parseTimestamp(doc.get("updatedAt")))
                .active(true)
                .version(1)
                .build();
                
        } catch (Exception e) {
            log.error("Error converting document to VectorRecord", e);
            return null;
        }
    }
    
    /**
     * Parse timestamp from string
     */
    private java.time.LocalDateTime parseTimestamp(String timestampStr) {
        try {
            if (timestampStr != null) {
                long timestamp = Long.parseLong(timestampStr);
                return java.time.LocalDateTime.ofInstant(
                    java.time.Instant.ofEpochMilli(timestamp),
                    java.time.ZoneId.systemDefault()
                );
            }
        } catch (Exception e) {
            log.warn("Error parsing timestamp: {}", timestampStr, e);
        }
        return java.time.LocalDateTime.now();
    }
    
    /**
     * Note: Similarity calculation is now handled by Lucene's native k-NN search.
     * This eliminates the need for manual cosine similarity calculations.
     * The VectorDatabaseService abstraction ensures that all implementations
     * (Lucene, Pinecone, Qdrant, etc.) handle similarity internally.
     */
    
    /**
     * Refresh the index reader and searcher
     */
    private void refreshReader() {
        try {
            if (indexReader != null) {
                indexReader.close();
            }
            
            // Check if index exists before trying to open it
            if (DirectoryReader.indexExists(directory)) {
                indexReader = DirectoryReader.open(directory);
            } else {
                // Index doesn't exist yet - commit writer to create empty index, then open reader
                if (indexWriter != null) {
                    indexWriter.commit();
                    indexReader = DirectoryReader.open(directory);
                } else {
                    // Create empty reader - open will fail for empty index, so we commit first
                    indexReader = DirectoryReader.open(directory);
                }
            }
            indexSearcher = new IndexSearcher(indexReader);
            
        } catch (Exception e) {
            log.error("Error refreshing Lucene reader", e);
            throw new AIServiceException("Failed to refresh Lucene reader", e);
        }
    }

    private IndexWriter recoverFromLock(Directory directory, IndexWriterConfig config, Path path) {
        final long timeoutMillis = 5000L;
        final long retryDelayMillis = 100L;
        long deadline = System.currentTimeMillis() + timeoutMillis;

        while (System.currentTimeMillis() < deadline) {
            try {
                Lock cleanupLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME);
                try {
                    cleanupLock.close();
                } catch (IOException closeException) {
                    log.warn("Error closing Lucene cleanup lock for {}", path, closeException);
                }
            } catch (LockObtainFailedException retryException) {
                log.warn("Lucene lock at {} still held; retrying in {} ms", path, retryDelayMillis);
                sleep(retryDelayMillis);
                continue;
            } catch (IOException ioException) {
                throw new UncheckedIOException(ioException);
            }

            try {
                return new IndexWriter(directory, config);
            } catch (LockObtainFailedException retryException) {
                log.warn("Unable to reopen Lucene index at {}; retrying in {} ms", path, retryDelayMillis, retryException);
                sleep(retryDelayMillis);
            } catch (IOException ioException) {
                throw new UncheckedIOException(ioException);
            }
        }

        throw new UncheckedIOException(new LockObtainFailedException("Unable to recover Lucene lock at " + path));
    }

    private void sleep(long millis) {
        try {
            Thread.sleep(millis);
        } catch (InterruptedException interruptedException) {
            Thread.currentThread().interrupt();
            throw new AIServiceException("Interrupted while waiting for Lucene lock recovery", interruptedException);
        }
    }

    private static class SharedIndex {
        private final Directory directory;
        private final IndexWriter writer;
        private final AtomicInteger refCount = new AtomicInteger(1);

        private SharedIndex(Directory directory, IndexWriter writer) {
            this.directory = directory;
            this.writer = writer;
        }

        private void retain() {
            refCount.incrementAndGet();
        }

        private boolean release() {
            return refCount.decrementAndGet() == 0;
        }
    }
}