# Production Configuration - Vector Database
ai:
  providers:
    # Embedding Provider Configuration
    # Options: onnx (local), rest (Docker service), openai (cloud API)
    embedding-provider: ${EMBEDDING_PROVIDER:openai}
    
    # ONNX Configuration (for local embeddings)
    onnx-model-path: ${ONNX_MODEL_PATH:./models/embeddings/all-MiniLM-L6-v2.onnx}
    onnx-tokenizer-path: ${ONNX_TOKENIZER_PATH:./models/embeddings/tokenizer.json}
    onnx-max-sequence-length: 512
    onnx-use-gpu: ${ONNX_USE_GPU:false}
    
    # REST Configuration (for Docker/sentence-transformers service)
    rest-base-url: ${REST_EMBEDDING_URL:http://localhost:8000}
    rest-endpoint: /embed
    rest-batch-endpoint: /embed/batch
    rest-timeout: 30000
    rest-model: all-MiniLM-L6-v2
    
    # OpenAI Configuration (production fallback)
    openai-api-key: ${OPENAI_API_KEY:}
    openai-embedding-model: text-embedding-3-small
    openai-timeout: 60
  
  vector-db:
    type: pinecone
    pinecone:
      apiKey: ${PINECONE_API_KEY:}
      environment: "us-east-1-aws"
      indexName: "ai-infrastructure-prod"
      dimensions: 1536
      metric: "cosine"
      pods: 2
      podType: "p1"
      enableMetadataFiltering: true

# Logging configuration for production
logging:
  level:
    com.ai.infrastructure.rag: INFO
    com.ai.infrastructure.service.VectorManagementService: INFO
    com.ai.infrastructure.service.AICapabilityService: INFO