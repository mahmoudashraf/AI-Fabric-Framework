name: Integration Tests (Manual Trigger)

on:
  workflow_dispatch:
    inputs:
      modules:
        description: 'Test modules to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - ai-infrastructure
          - behavior-analytics
          - relationship-query
      openai_api_key:
        description: 'OpenAI API Key'
        required: true
        type: string
      llm_provider:
        description: 'LLM Provider'
        required: true
        default: 'openai'
        type: choice
        options:
          - openai
          - azure-openai
          - cohere
          - anthropic
          - rest
      embedding_provider:
        description: 'Embedding Provider'
        required: true
        default: 'openai'
        type: choice
        options:
          - openai
          - azure-openai
          - onnx
      vector_database:
        description: 'Vector Database'
        required: true
        default: 'lucene'
        type: choice
        options:
          - lucene
          - pinecone
          - weaviate
          - qdrant
          - milvus
          - memory
      persistence_database:
        description: 'Persistence Database'
        required: true
        default: 'h2'
        type: choice
        options:
          - h2
          - postgresql
      timeout_minutes:
        description: 'Job timeout in minutes'
        required: false
        default: '30'

jobs:
  # ==============================================================================
  # Job 1: AI Infrastructure Integration Tests
  # ==============================================================================
  ai-infrastructure-tests:
    name: AI Infrastructure Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: ${{ fromJSON(github.event.inputs.timeout_minutes) }}
    if: |
      github.event.inputs.modules == 'all' || 
      github.event.inputs.modules == 'ai-infrastructure'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up JDK 21
        uses: actions/setup-java@v4
        with:
          java-version: '21'
          distribution: 'temurin'
          cache: 'maven'

      - name: Cache Maven packages
        uses: actions/cache@v3
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-maven-ai-infra-${{ hashFiles('**/pom.xml') }}
          restore-keys: |
            ${{ runner.os }}-maven-ai-infra-
            ${{ runner.os }}-maven-

      - name: Build AI Infrastructure Module
        run: |
          cd ai-infrastructure-module
          mvn clean verify -B -V

      - name: Run Integration Tests (Real API)
        run: |
          cd ai-infrastructure-module/integration-tests
          bash run-provider-matrix-tests.sh "${{ github.event.inputs.llm_provider }}:${{ github.event.inputs.embedding_provider }}:${{ github.event.inputs.vector_database }}"
        env:
          TESTCONTAINERS_RYUK_DISABLED: false
          OPENAI_API_KEY: ${{ github.event.inputs.openai_api_key }}
          AI_INFRASTRUCTURE_PERSISTENCE_DATABASE: ${{ github.event.inputs.persistence_database }}

      - name: Upload test reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ai-infrastructure-test-reports
          path: |
            ai-infrastructure-module/integration-tests/target/surefire-reports/
            ai-infrastructure-module/integration-tests/target/failsafe-reports/
          retention-days: 30

      - name: Publish Test Results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always()
        with:
          files: |
            ai-infrastructure-module/integration-tests/target/surefire-reports/*.xml
          check_name: AI Infrastructure Test Results

  # ==============================================================================
  # Job 2: Behavior Analytics Integration Tests
  # ==============================================================================
  behavior-analytics-tests:
    name: Behavior Analytics Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: |
      github.event.inputs.modules == 'all' || 
      github.event.inputs.modules == 'behavior-analytics'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up JDK 21
        uses: actions/setup-java@v4
        with:
          java-version: '21'
          distribution: 'temurin'
          cache: 'maven'

      - name: Cache Maven packages
        uses: actions/cache@v3
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-maven-behavior-${{ hashFiles('**/pom.xml') }}
          restore-keys: |
            ${{ runner.os }}-maven-behavior-
            ${{ runner.os }}-maven-

      - name: Build AI Infrastructure Module
        run: |
          cd ai-infrastructure-module
          mvn clean install -DskipTests -B -V

      - name: Run Behavior Analytics Integration Tests
        run: |
          cd ai-infrastructure-module
          mvn test -pl ai-infrastructure-behavior-integration-tests -B
        env:
          TESTCONTAINERS_RYUK_DISABLED: false
          SPRING_PROFILES_ACTIVE: test
          OPENAI_API_KEY: ${{ github.event.inputs.openai_api_key }}
          AI_INFRASTRUCTURE_LLM_PROVIDER: ${{ github.event.inputs.llm_provider }}
          AI_INFRASTRUCTURE_EMBEDDING_PROVIDER: ${{ github.event.inputs.embedding_provider }}
          AI_INFRASTRUCTURE_VECTOR_DATABASE: ${{ github.event.inputs.vector_database }}
          AI_INFRASTRUCTURE_PERSISTENCE_DATABASE: ${{ github.event.inputs.persistence_database }}

      - name: Upload test reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: behavior-analytics-test-reports
          path: |
            ai-infrastructure-module/ai-infrastructure-behavior-integration-tests/target/surefire-reports/
            ai-infrastructure-module/ai-infrastructure-behavior-integration-tests/target/failsafe-reports/
          retention-days: 30

      - name: Publish Test Results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always()
        with:
          files: |
            ai-infrastructure-module/ai-infrastructure-behavior-integration-tests/target/surefire-reports/*.xml
          check_name: Behavior Analytics Test Results

  # ==============================================================================
  # Job 3: Relationship Query Integration Tests
  # ==============================================================================
  relationship-query-tests:
    name: Relationship Query Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    if: |
      github.event.inputs.modules == 'all' || 
      github.event.inputs.modules == 'relationship-query'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up JDK 21
        uses: actions/setup-java@v4
        with:
          java-version: '21'
          distribution: 'temurin'
          cache: 'maven'

      - name: Cache Maven packages
        uses: actions/cache@v3
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-maven-relationship-${{ hashFiles('**/pom.xml') }}
          restore-keys: |
            ${{ runner.os }}-maven-relationship-
            ${{ runner.os }}-maven-

      - name: Build AI Infrastructure Module
        run: |
          cd ai-infrastructure-module
          mvn clean install -DskipTests -B -V

      - name: Run Relationship Query Integration Tests (Real API)
        run: |
          cd ai-infrastructure-module/relationship-query-integration-tests
          bash run-relationship-query-realapi-tests.sh
        env:
          OPENAI_API_KEY: ${{ github.event.inputs.openai_api_key }}
          AI_INFRASTRUCTURE_LLM_PROVIDER: ${{ github.event.inputs.llm_provider }}
          AI_INFRASTRUCTURE_EMBEDDING_PROVIDER: ${{ github.event.inputs.embedding_provider }}
          AI_INFRASTRUCTURE_VECTOR_DATABASE: ${{ github.event.inputs.vector_database }}
          AI_INFRASTRUCTURE_PERSISTENCE_DATABASE: ${{ github.event.inputs.persistence_database }}

      - name: Upload test reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: relationship-query-test-reports
          path: |
            ai-infrastructure-module/relationship-query-integration-tests/target/surefire-reports/
            ai-infrastructure-module/relationship-query-integration-tests/target/failsafe-reports/
          retention-days: 30

      - name: Publish Test Results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always()
        with:
          files: |
            ai-infrastructure-module/relationship-query-integration-tests/target/surefire-reports/*.xml
          check_name: Relationship Query Test Results

  # ==============================================================================
  # Summary Job - Aggregates all test results
  # ==============================================================================
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [ai-infrastructure-tests, behavior-analytics-tests, relationship-query-tests]
    if: always()
    
    steps:
      - name: Download all test reports
        uses: actions/download-artifact@v4
        with:
          path: test-reports

      - name: Create Test Summary
        run: |
          echo "# üß™ Integration Tests Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Triggered by:** @${{ github.actor }}" >> $GITHUB_STEP_SUMMARY
          echo "**Modules tested:** ${{ github.event.inputs.modules }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Configuration" >> $GITHUB_STEP_SUMMARY
          echo "- **LLM Provider:** ${{ github.event.inputs.llm_provider }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Embedding Provider:** ${{ github.event.inputs.embedding_provider }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Vector Database:** ${{ github.event.inputs.vector_database }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Persistence Database:** ${{ github.event.inputs.persistence_database }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Check job statuses
          AI_INFRA_STATUS="${{ needs.ai-infrastructure-tests.result }}"
          BEHAVIOR_STATUS="${{ needs.behavior-analytics-tests.result }}"
          RELATIONSHIP_STATUS="${{ needs.relationship-query-tests.result }}"
          
          # Function to get emoji for status
          get_emoji() {
            case $1 in
              success) echo "‚úÖ" ;;
              failure) echo "‚ùå" ;;
              skipped) echo "‚è≠Ô∏è" ;;
              cancelled) echo "üö´" ;;
              *) echo "‚ùì" ;;
            esac
          }
          
          echo "| Module | Status | Result |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|--------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| AI Infrastructure | $(get_emoji $AI_INFRA_STATUS) | $AI_INFRA_STATUS |" >> $GITHUB_STEP_SUMMARY
          echo "| Behavior Analytics | $(get_emoji $BEHAVIOR_STATUS) | $BEHAVIOR_STATUS |" >> $GITHUB_STEP_SUMMARY
          echo "| Relationship Query | $(get_emoji $RELATIONSHIP_STATUS) | $RELATIONSHIP_STATUS |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Overall result
          if [[ "$AI_INFRA_STATUS" == "success" || "$AI_INFRA_STATUS" == "skipped" ]] && \
             [[ "$BEHAVIOR_STATUS" == "success" || "$BEHAVIOR_STATUS" == "skipped" ]] && \
             [[ "$RELATIONSHIP_STATUS" == "success" || "$RELATIONSHIP_STATUS" == "skipped" ]]; then
            echo "### ‚úÖ All tests passed!" >> $GITHUB_STEP_SUMMARY
          else
            echo "### ‚ùå Some tests failed. Please check the logs." >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "üìä **View detailed reports in the artifacts section**" >> $GITHUB_STEP_SUMMARY

      - name: Check overall test status
        run: |
          AI_INFRA_STATUS="${{ needs.ai-infrastructure-tests.result }}"
          BEHAVIOR_STATUS="${{ needs.behavior-analytics-tests.result }}"
          RELATIONSHIP_STATUS="${{ needs.relationship-query-tests.result }}"
          
          if [[ "$AI_INFRA_STATUS" == "failure" ]] || \
             [[ "$BEHAVIOR_STATUS" == "failure" ]] || \
             [[ "$RELATIONSHIP_STATUS" == "failure" ]]; then
            echo "‚ùå One or more test jobs failed"
            exit 1
          else
            echo "‚úÖ All test jobs passed or were skipped"
          fi
